{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffba575a",
   "metadata": {},
   "source": [
    "# CarPK preprocessing\n",
    "\n",
    "Authors of FamNet tested their model on category specific dataset CarPK which contains pictures taken from above (with drone) of cars in the parking lot. As the ground truths in this dataset are provided as bounding boxes, they need to be transformed into the density maps as required by FamNet. Additionaly this dataset was not meant for few-shot learning, therefore exemplars have to be created as well (some of the bounding boxes should be choosen as exemplars). However the authors of FamNet do not provide the code for CarPK experiment, so we have to do that preprocessing by ourselves.\n",
    "\n",
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c280e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# We go to parent directory so we can import utils\n",
    "os.chdir(\"./..\")\n",
    "from utils import matlab_style_gauss2D\n",
    "\n",
    "RANDOM_SEED = 7\n",
    "DATA_PATH = \"data/CARPK_devkit/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4445ae73",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Here we first provide an example of a bounding box annotation and of our density map generation\n",
    "\n",
    "### Bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9b4260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9fbee46f10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(DATA_PATH + \"Images/20160331_NTU_00001.png\")\n",
    "boxes = []\n",
    "with open(DATA_PATH + \"Annotations/20160331_NTU_00001.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        coords = line.split(\" \")\n",
    "        x1 = int(coords[0])\n",
    "        y1 = int(coords[1])\n",
    "        x2 = int(coords[2])\n",
    "        y2 = int(coords[3])\n",
    "        boxes.append([x1, y1, x2, y2])\n",
    "\n",
    "first_box = boxes[0]\n",
    "cv2.rectangle(img, (first_box[0], first_box[1]), (first_box[2], first_box[3]), (128, 255, 0), 4)\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8978ad09",
   "metadata": {},
   "source": [
    "### Density map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b75365a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9fbedaff70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get image dimensions and create the density map of same size\n",
    "h, w, _ = img.shape\n",
    "density_map = np.zeros((h, w), dtype=\"float32\")\n",
    "\n",
    "# Add gaussian density for each bounding box\n",
    "for box in boxes:\n",
    "    # Get the dimension of bounding box\n",
    "    w_box, h_box = box[2] - box[0], box[3] - box[1]\n",
    "    # Get the gaussian density on that rectangle (as sigma we take 1/8 of the dimension average)\n",
    "    density = matlab_style_gauss2D(shape=(h_box, w_box), sigma=(h_box + w_box) / 16)\n",
    "    #print(density)\n",
    "    # Add that density to density map\n",
    "    density_map[box[1]:box[3], box[0]:box[2]] = density\n",
    "\n",
    "plt.imshow(density_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32017576",
   "metadata": {},
   "source": [
    "## Estimate the number of exemplars distribution\n",
    "\n",
    "If we want to choose the exemplars in CarPK images appropriately, we first need to estimate how the number of exemplars is distributed in the FSC-147 dataset and then sample a number of exemplars for given CarPK image from that distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0fd81d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated probability distribution of number of exemplars for FSC-147 dataset (Number: probability)\n",
      "0: 0.0\n",
      "1: 0.0\n",
      "2: 0.0\n",
      "3: 9.625772860397006e-01\n",
      "4: 0.034493979824275954\n",
      "5: 0.0027660266840221284\n",
      "6: 0.00016270745200130165\n"
     ]
    }
   ],
   "source": [
    "# List of all images in FSC-147 dataset\n",
    "im_list = os.listdir(\"data/images_384_VarV2\")\n",
    "\n",
    "# FSC-147 annotations\n",
    "with open(\"data/annotation_FSC147_384.json\") as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Go through all images and get number of exemplars for each\n",
    "n_exemplars = []\n",
    "for img in im_list:\n",
    "    annot = annotations[img]\n",
    "    n_exemplars.append(len(annot[\"box_examples_coordinates\"]))\n",
    "\n",
    "# Get the density estimations\n",
    "hist = np.histogram(n_exemplars, bins=np.arange(max(n_exemplars)+2), density=True)\n",
    "\n",
    "print(\"Estimated probability distribution of number of exemplars for FSC-147 dataset (Number: probability)\")\n",
    "probs, numbers = hist\n",
    "# Remove the last number as it was there just so the densities were calculated correctly\n",
    "numbers = numbers[:-1]\n",
    "for p, n in zip(probs, numbers):\n",
    "    print(str(n)+\":\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908ed8be",
   "metadata": {},
   "source": [
    "## Create ground truths and exemplars, image resizing\n",
    "\n",
    "Here we create ground truth density maps and select exemplars that will be used by FamNet. Since we have all cars annotated with bounding boxes, we can just use a Gaussian filter of same dimension as bounding box to create a density for specific cars. For sigma we use 1/8 of the average of height and width of the bounding box, as this means that sigma is chosen in the similar way as in the FSC-147 dataset. To choose the exemplars, we first randomly sample the number of exemplars from the distribution estimated above. If the chosen number of exemplars is too big (there are not enough cars on the image), we change the number of exemplars to the half of the cars in the image. When we have the number of exemplars, we just randomly choose exemplars from given bounding boxes.\n",
    "\n",
    "We also resize the images to height 384, as FamNet was trained with such resized images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8063681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes dimensions and bounding boxes that represent cars and return the density map\n",
    "def create_density_map(h, w, boxes):\n",
    "    # Get image dimensions and create the density map of same size\n",
    "    h, w, _ = img.shape\n",
    "    density_map = np.zeros((h, w), dtype=\"float32\")\n",
    "\n",
    "    # Add gaussian density for each bounding box\n",
    "    for box in boxes:\n",
    "        # Get the dimension of bounding box\n",
    "        w_box, h_box = box[2] - box[0], box[3] - box[1]\n",
    "        # Get the gaussian density on that rectangle (as sigma we take 1/8 of the dimension average)\n",
    "        density = matlab_style_gauss2D(shape=(h_box, w_box), sigma=(h_box + w_box) / 16)\n",
    "        #print(density)\n",
    "        # Add that density to density map\n",
    "        density_map[box[1]:box[3], box[0]:box[2]] = density\n",
    "\n",
    "    return density_map\n",
    "\n",
    "# Function that takes .txt annotation file and return the bounding boxes for cars\n",
    "def read_annotation(filename):\n",
    "    boxes = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            coords = line.split(\" \")\n",
    "            x1 = int(coords[0])\n",
    "            y1 = int(coords[1])\n",
    "            x2 = int(coords[2])\n",
    "            y2 = int(coords[3])\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "\n",
    "    return boxes\n",
    "\n",
    "# Function that resize original bounding boxes to match the resized image\n",
    "def resize_boxes(boxes, resize_ratio):\n",
    "    resized_boxes = []\n",
    "    for box in boxes:\n",
    "        resized_boxes.append([int(round(value * resize_ratio)) for value in box])\n",
    "\n",
    "    return resized_boxes\n",
    "\n",
    "# Function that transforms bounding boxes given by two points into FSC-147 JSON format with 4 points\n",
    "def to_json_boxes(boxes):\n",
    "    transformed_boxes = []\n",
    "    for box in boxes:\n",
    "        transformed_boxes.append([[box[0], box[1]], [box[0], box[3]], [box[2], box[3]], [box[2], box[1]]])\n",
    "\n",
    "    return transformed_boxes\n",
    "\n",
    "# Function that returns the center points of given bounding boxes\n",
    "def get_centers(boxes):\n",
    "    return [[(box[0] + box[2]) / 2, (box[1] + box[3]) / 2] for box in boxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa6a95a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672f067339a14846a62ac786e97e1c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1448 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the list of all images in CarPK dataset\n",
    "img_list = os.listdir(DATA_PATH + \"Images\")\n",
    "\n",
    "# Dictionary for FSC-147 style annotations that will be created\n",
    "transformed_annotations = {}\n",
    "\n",
    "# Create the directory for transformed images if it does not exist already\n",
    "if not os.path.exists(DATA_PATH + \"Resized_images\"):\n",
    "    os.mkdir(DATA_PATH + \"Resized_images\")\n",
    "\n",
    "# Set the random seed (for reproducibility)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "for im_name in tqdm(img_list):\n",
    "    # Annotation for this image that will be stored in JSON file\n",
    "    new_annotation = {}\n",
    "\n",
    "    # Read the image and get its dimensions\n",
    "    img = cv2.imread(DATA_PATH + \"Images/\" + im_name)\n",
    "    h, w, _ = img.shape\n",
    "    # Store the original dimensions in annotations as in the case of FSC-147 dataset\n",
    "    new_annotation[\"H\"] = h\n",
    "    new_annotation[\"W\"] = w\n",
    "\n",
    "    # Get the bounding boxes\n",
    "    boxes = read_annotation(DATA_PATH + \"Annotations/\" + im_name.rstrip(\"png\") + \"txt\")\n",
    "\n",
    "    # Resize the image to height 384\n",
    "    resize_ratio = 384 / h\n",
    "    new_h = 384\n",
    "    new_w = int(round(w*resize_ratio))\n",
    "    img = cv2.resize(img, dsize=(new_w, new_h))\n",
    "    # Save the resized image\n",
    "    cv2.imwrite(DATA_PATH + \"Resized_images/\" + im_name, img)\n",
    "    # Store the resize ratio\n",
    "    new_annotation[\"ratio_h\"] = resize_ratio\n",
    "    new_annotation[\"ratio_w\"] = resize_ratio\n",
    "\n",
    "    # Transform the bounding boxes for resized image\n",
    "    boxes = resize_boxes(boxes, resize_ratio)\n",
    "\n",
    "    # Select the exemplars\n",
    "    n_exemplars = np.random.choice(numbers, p=probs)\n",
    "    if len(boxes) <= n_exemplars:\n",
    "        n_exemplars = len(boxes) // 2\n",
    "    exemplar_idcs = np.random.choice(np.arange(len(boxes)), size=n_exemplars, replace=False)\n",
    "    exemplars = [boxes[i] for i in exemplar_idcs]\n",
    "    # Store the exemplars\n",
    "    new_annotation[\"box_examples_coordinates\"] = to_json_boxes(exemplars)\n",
    "\n",
    "    # Get the center of each bounding box and store it as in the case of FSC-147 dataset\n",
    "    dots = get_centers(boxes)\n",
    "    # Store the centers\n",
    "    new_annotation[\"points\"] = dots\n",
    "\n",
    "    transformed_annotations[im_name] = new_annotation\n",
    "\n",
    "with open(DATA_PATH + \"annotations.json\", 'w') as f:\n",
    "    json.dump(transformed_annotations, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d3321e",
   "metadata": {},
   "source": [
    "## Transform the data splits\n",
    "\n",
    "We also transform the train-test split in JSON form so it is the same as in FSC-147 case, so the experiment can be easilly run with the train/test script for FSC-147 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96f7b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = {}\n",
    "\n",
    "for split in [\"train\", \"test\"]:\n",
    "    with open(DATA_PATH + \"ImageSets/\" + split + \".txt\", \"r\") as f:\n",
    "        imgs = []\n",
    "        for line in f:\n",
    "            im_name = line.strip()\n",
    "            imgs.append(im_name + \".png\")\n",
    "\n",
    "        data_split[split] = imgs\n",
    "\n",
    "with open(DATA_PATH + \"train_test.json\", 'w') as f:\n",
    "    json.dump(data_split, f, indent=4, sort_keys=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reproducibility",
   "language": "python",
   "name": "reproducibility"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
